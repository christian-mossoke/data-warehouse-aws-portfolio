ğŸ§± Data Warehouse AWS Portfolio Project
ğŸš€ Objectif du projet

Ce projet fait partie dâ€™une sÃ©rie de projets data engineering destinÃ©s Ã  mon portfolio professionnel.
Lâ€™objectif est de concevoir, dÃ©ployer et automatiser une data warehouse complÃ¨te sur AWS, en partant de fichiers CSV locaux jusquâ€™Ã  une infrastructure cloud orchestrÃ©e par AWS Step Functions.

ğŸ—‚ï¸ Architecture du projet
1. Sources de donnÃ©es

Quatre fichiers CSV simulant un environnement e-commerce :

clients.csv

produits.csv

commandes.csv

lignes_commandes.csv

Ces fichiers contiennent les donnÃ©es brutes servant de base Ã  la Data Warehouse.

2. Ã‰tapes principales

Ingestion locale et prÃ©paration des donnÃ©es

Nettoyage et transformation avec extract-transform-load.py

Chargement dans une base Postgres locale (datawarehouse)

Migration vers AWS

Les fichiers sont transfÃ©rÃ©s dans un bucket S3 structurÃ© :

s3://datawarehouse-portfolio/
  â”œâ”€â”€ raw/
  â”œâ”€â”€ processed/
  â””â”€â”€ glue_scripts/


DÃ©ploiement dâ€™infrastructure avec Terraform

Bucket S3

Base de donnÃ©es AWS Glue Data Catalog

RÃ´le IAM pour Glue & Step Functions

Job AWS Glue (etl_glue_script.py)

Orchestration avec AWS Step Functions

Exploration et analyse

RequÃªtes SQL sur S3 avec Amazon Athena

Vue logique de la Data Warehouse via tables dimensionnelles et fact tables

ğŸ§° Stack technique
Domaine	Outil / Technologie
Langage principal	Python 3
Cloud	AWS
Infrastructure as Code	Terraform
ETL	AWS Glue (PySpark)
Orchestration	AWS Step Functions
Stockage	Amazon S3
RequÃªtage analytique	Amazon Athena
Base de test locale	PostgreSQL
IDE	VS Code
Monitoring & visualisation	DBeaver / AWS Console
ğŸ“ Structure du projet
data-warehouse-aws-portfolio/
â”‚
â”œâ”€â”€ data/                         # Fichiers CSV bruts
â”‚   â”œâ”€â”€ clients.csv
â”‚   â”œâ”€â”€ produits.csv
â”‚   â”œâ”€â”€ commandes.csv
â”‚   â””â”€â”€ lignes_commandes.csv
â”‚
â”œâ”€â”€ sql/                          # Scripts SQL
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â””â”€â”€ queries/
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ extract-transform-load.py # Pipeline ETL local
â”‚
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ main.tf
â”‚       â”œâ”€â”€ s3.tf
â”‚       â”œâ”€â”€ glue.tf
â”‚       â”œâ”€â”€ iam.tf
â”‚       â”œâ”€â”€ stepfunction.tf
â”‚       â”œâ”€â”€ variables.tf
â”‚       â”œâ”€â”€ outputs.tf
â”‚       â””â”€â”€ glue_scripts/
â”‚           â””â”€â”€ etl_glue_script.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

âš™ï¸ Instructions dâ€™exÃ©cution
Ã‰tape 1 â€” Local

CrÃ©er lâ€™environnement :

python -m venv venv
source venv/bin/activate  # ou venv\Scripts\activate sur Windows


Installer les dÃ©pendances :

pip install -r requirements.txt


CrÃ©er la base de donnÃ©es locale :

psql -U postgres -c "CREATE DATABASE datawarehouse;"


CrÃ©er les tables :

psql -U postgres -d datawarehouse -f sql/create_tables.sql


Lancer le script ETL :

python scripts/extract-transform-load.py

Ã‰tape 2 â€” AWS Infrastructure (Terraform)

Aller dans le dossier Terraform :

cd infra/terraform


Initialiser Terraform :

terraform init


VÃ©rifier le plan :

terraform plan


Appliquer le dÃ©ploiement :

terraform apply

Ã‰tape 3 â€” Orchestration avec Step Functions

Le workflow exÃ©cute :

Le job AWS Glue pour traiter les donnÃ©es brutes â†’ donnÃ©es transformÃ©es

La mise Ã  jour du catalogue Glue

La validation finale du job

Ã‰tape 4 â€” Analyse

ExÃ©cuter des requÃªtes SQL dans Amazon Athena :

SELECT produit_id, SUM(total) AS total_ventes
FROM processed.commandes
GROUP BY produit_id
ORDER BY total_ventes DESC;

ğŸ§  CompÃ©tences dÃ©montrÃ©es

ModÃ©lisation de Data Warehouse (star schema)

Conception de pipeline ETL (local â†’ cloud)

Infrastructure as Code avec Terraform

Orchestration serverless (AWS Step Functions)

DÃ©ploiement AWS Glue Job

Analyse avec Athena

Automatisation de bout en bout (CI/CD possible)

ğŸŒ Auteur

Christian Mossoke
ğŸ’¼ Data Engineer 
ğŸ“§ Contact : christian.mossoke.pro@gmail.com